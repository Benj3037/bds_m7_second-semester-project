{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"font-width:bold; font-size: 3rem; color:#2656a3;\">**Msc. BDS - M7 Second Semester Project** </span> <span style=\"font-width:bold; font-size: 3rem; color:#333;\">- Part 03: Training Pipeline</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color:#2656a3'> üóíÔ∏è This notebook is divided into the following sections:\n",
    "1. Feature selection.\n",
    "2. Creating a Feature View.\n",
    "3. Training datasets creation - splitting into train and test sets.\n",
    "4. Training the model.\n",
    "5. Register the model to Hopsworks Model Registry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color:#2656a3'> ‚öôÔ∏è Import of libraries and packages\n",
    "We start with importing some of the necessary libraries needed for this notebook and warnings to avoid unnecessary distractions and keep output clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the packages and libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#2656a3;\"> üì° Connecting to Hopsworks Feature Store\n",
    "We connect to Hopsworks Feature Store so we can retrieve the Feature Groups and select features for training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/554133\n",
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    }
   ],
   "source": [
    "# Importing the hopsworks module for interacting with the Hopsworks platform\n",
    "import hopsworks\n",
    "\n",
    "# Logging into the Hopsworks project\n",
    "project = hopsworks.login()\n",
    "\n",
    "# Getting the feature store from the project\n",
    "fs = project.get_feature_store() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the feature groups\n",
    "electricity_fg = fs.get_feature_group(\n",
    "    name='electricity_prices',\n",
    "    version=1,\n",
    ")\n",
    "\n",
    "electricity_price_window_fg = fs.get_feature_group(\n",
    "    name='electricity_price_window',\n",
    "    version=1,\n",
    ")\n",
    "\n",
    "weather_fg = fs.get_feature_group(\n",
    "    name='weather_measurements',\n",
    "    version=1,\n",
    ")\n",
    "\n",
    "danish_calendar_fg = fs.get_feature_group(\n",
    "    name='dk_calendar',\n",
    "    version=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#2656a3;\"> üñç Feature View Creation and Retrieving </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first select the features that we want to include for model training.\n",
    "\n",
    "Since we specified `primary_key`as `date` and `timestamp` in `1_feature_backfill` we can now join them together for the `electricity_fg`, `weather_fg` and `danish_holiday_fg`.\n",
    "\n",
    "`join_type` specifies the type of join to perform. An inner join refers to only retaining the rows based on the keys present in all joined DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for training data and join them together and except duplicate columns\n",
    "selected_features = electricity_fg.select_all()\\\n",
    "    .join(electricity_price_window_fg.select_except([\"timestamp\"]), join_type=\"inner\")\\\n",
    "    .join(weather_fg.select_except([\"timestamp\", \"datetime\", \"hour\", \"date\"]), join_type=\"inner\")\\\n",
    "    .join(danish_calendar_fg.select_except([\"timestamp\", \"datetime\", \"hour\", \"date\"]), join_type=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformation_functions = {\n",
    "#         \"hour\": fs.get_transformation_function(name=\"min_max_scaler\"),\n",
    "#         \"dk1_spotpricedkk_kwh\": fs.get_transformation_function(name=\"min_max_scaler\"),\n",
    "#         \"temperature_2m\": fs.get_transformation_function(name=\"min_max_scaler\"),\n",
    "#         \"relative_humidity_2m\": fs.get_transformation_function(name=\"min_max_scaler\"),\n",
    "#         \"precipitation\": fs.get_transformation_function(name=\"min_max_scaler\"),\n",
    "#         \"rain\": fs.get_transformation_function(name=\"min_max_scaler\"),\n",
    "#         \"snowfall\": fs.get_transformation_function(name=\"min_max_scaler\"),\n",
    "#         \"weather_code\": fs.get_transformation_function(name=\"min_max_scaler\"),\n",
    "#         \"cloud_cover\": fs.get_transformation_function(name=\"min_max_scaler\"),\n",
    "#         \"wind_speed_10m\": fs.get_transformation_function(name=\"min_max_scaler\"),\n",
    "#         \"wind_gusts_10m\": fs.get_transformation_function(name=\"min_max_scaler\"),\n",
    "#         \"dayofweek\": fs.get_transformation_function(name=\"min_max_scaler\"),\n",
    "#         \"day\": fs.get_transformation_function(name=\"min_max_scaler\"),\n",
    "#         \"month\": fs.get_transformation_function(name=\"min_max_scaler\"),\n",
    "#         \"year\": fs.get_transformation_function(name=\"min_max_scaler\"),\n",
    "#         \"workday\": fs.get_transformation_function(name=\"min_max_scaler\"),\n",
    "#     }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using ArrowFlight (4.43s) \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>datetime</th>\n",
       "      <th>date</th>\n",
       "      <th>hour</th>\n",
       "      <th>dk1_spotpricedkk_kwh</th>\n",
       "      <th>prev_1w_mean</th>\n",
       "      <th>prev_2w_mean</th>\n",
       "      <th>prev_4w_mean</th>\n",
       "      <th>temperature_2m</th>\n",
       "      <th>relative_humidity_2m</th>\n",
       "      <th>...</th>\n",
       "      <th>snowfall</th>\n",
       "      <th>weather_code</th>\n",
       "      <th>cloud_cover</th>\n",
       "      <th>wind_speed_10m</th>\n",
       "      <th>wind_gusts_10m</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>workday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1692856800000</td>\n",
       "      <td>2023-08-24 06:00:00+00:00</td>\n",
       "      <td>2023-08-24 00:00:00+00:00</td>\n",
       "      <td>6</td>\n",
       "      <td>1.22897</td>\n",
       "      <td>0.795215</td>\n",
       "      <td>0.729342</td>\n",
       "      <td>0.566330</td>\n",
       "      <td>14.8</td>\n",
       "      <td>77.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>17.7</td>\n",
       "      <td>32.8</td>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>8</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1695733200000</td>\n",
       "      <td>2023-09-26 13:00:00+00:00</td>\n",
       "      <td>2023-09-26 00:00:00+00:00</td>\n",
       "      <td>13</td>\n",
       "      <td>0.61856</td>\n",
       "      <td>0.458488</td>\n",
       "      <td>0.540868</td>\n",
       "      <td>0.662041</td>\n",
       "      <td>19.4</td>\n",
       "      <td>66.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>12.4</td>\n",
       "      <td>25.9</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>9</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1657242000000</td>\n",
       "      <td>2022-07-08 01:00:00+00:00</td>\n",
       "      <td>2022-07-08 00:00:00+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1.53483</td>\n",
       "      <td>1.791675</td>\n",
       "      <td>1.946746</td>\n",
       "      <td>1.756484</td>\n",
       "      <td>12.9</td>\n",
       "      <td>87.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>33.8</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1691539200000</td>\n",
       "      <td>2023-08-09 00:00:00+00:00</td>\n",
       "      <td>2023-08-09 00:00:00+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.00842</td>\n",
       "      <td>0.328630</td>\n",
       "      <td>0.434574</td>\n",
       "      <td>0.447150</td>\n",
       "      <td>11.3</td>\n",
       "      <td>87.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>60.1</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1666929600000</td>\n",
       "      <td>2022-10-28 04:00:00+00:00</td>\n",
       "      <td>2022-10-28 00:00:00+00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>0.66365</td>\n",
       "      <td>0.872462</td>\n",
       "      <td>1.027621</td>\n",
       "      <td>1.074615</td>\n",
       "      <td>14.7</td>\n",
       "      <td>94.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>17.9</td>\n",
       "      <td>34.6</td>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>10</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       timestamp                  datetime                      date  hour  \\\n",
       "0  1692856800000 2023-08-24 06:00:00+00:00 2023-08-24 00:00:00+00:00     6   \n",
       "1  1695733200000 2023-09-26 13:00:00+00:00 2023-09-26 00:00:00+00:00    13   \n",
       "2  1657242000000 2022-07-08 01:00:00+00:00 2022-07-08 00:00:00+00:00     1   \n",
       "3  1691539200000 2023-08-09 00:00:00+00:00 2023-08-09 00:00:00+00:00     0   \n",
       "4  1666929600000 2022-10-28 04:00:00+00:00 2022-10-28 00:00:00+00:00     4   \n",
       "\n",
       "   dk1_spotpricedkk_kwh  prev_1w_mean  prev_2w_mean  prev_4w_mean  \\\n",
       "0               1.22897      0.795215      0.729342      0.566330   \n",
       "1               0.61856      0.458488      0.540868      0.662041   \n",
       "2               1.53483      1.791675      1.946746      1.756484   \n",
       "3              -0.00842      0.328630      0.434574      0.447150   \n",
       "4               0.66365      0.872462      1.027621      1.074615   \n",
       "\n",
       "   temperature_2m  relative_humidity_2m  ...  snowfall  weather_code  \\\n",
       "0            14.8                  77.0  ...       0.0           0.0   \n",
       "1            19.4                  66.0  ...       0.0           1.0   \n",
       "2            12.9                  87.0  ...       0.0           0.0   \n",
       "3            11.3                  87.0  ...       0.0          53.0   \n",
       "4            14.7                  94.0  ...       0.0           3.0   \n",
       "\n",
       "   cloud_cover  wind_speed_10m  wind_gusts_10m  dayofweek  day  month  year  \\\n",
       "0         10.0            17.7            32.8          3   24      8  2023   \n",
       "1         40.0            12.4            25.9          1   26      9  2023   \n",
       "2          6.0            18.0            33.8          4    8      7  2022   \n",
       "3        100.0            35.0            60.1          2    9      8  2023   \n",
       "4        100.0            17.9            34.6          4   28     10  2022   \n",
       "\n",
       "   workday  \n",
       "0        1  \n",
       "1        1  \n",
       "2        1  \n",
       "3        1  \n",
       "4        1  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first 5 rows of the selected features\n",
    "selected_features.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `Feature View` stands between the **Feature Groups** and **Training Dataset**. –°ombining **Feature Groups** we can create a **Feature View** which stores a metadata of our data. Having the **Feature View** we can create a **Training Dataset**.\n",
    "\n",
    "In order to create Feature View we can use `fs.get_or_create_feature_view()` method.\n",
    "\n",
    "We can specify parameters:\n",
    "\n",
    "- `name` - Name of the feature view to create.\n",
    "- `version` - Version of the feature view to create.\n",
    "- `query` - Query object with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting or creating a feature view named 'electricity_price_feature_view'\n",
    "version=1\n",
    "feature_view_training = fs.get_or_create_feature_view(\n",
    "    name='electricity_price_feature_view',\n",
    "    version=version,\n",
    "    query=selected_features,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Getting or creating a feature view named 'dk1_electricity_training_feature_view'\n",
    "# version = 1\n",
    "# feature_view_training = fs.get_or_create_feature_view(\n",
    "#     name='lstm_dk1_electricity_training_feature_view',\n",
    "#     version=version,\n",
    "#     transformation_functions=transformation_functions,\n",
    "#     query=selected_features_training,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#2656a3;\"> üèãÔ∏è Training Dataset Creation</span>\n",
    "\n",
    "In Hopsworks, a training dataset is generated from a query defined by the parent FeatureView, which determines the set of features.\n",
    "\n",
    "**Training Dataset may contain splits such as:** \n",
    "* Training set: This subset of the training data is utilized for model training.\n",
    "* Validation set: Used for evaluating hyperparameters during model training. *(We have not included a validation set for this project)*\n",
    "* Test set: Reserved as a holdout subset of training data for evaluating a trained model's performance.\n",
    "\n",
    "Training dataset is created using `fs.training_data()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve training data from the feature view 'feature_view_training', assigning the features to 'X'.\n",
    "df, _ = feature_view_training.training_data(\n",
    "    description = 'LSTM Electricity Prices Training Dataset',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the data by timestamp and reset the index for time series data\n",
    "df.sort_values(by='timestamp', ascending=True, inplace=True)\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#2656a3;\">üß¨ Modeling</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#2656a3;\">üß¨ Model 1: Random Train/Test Split LSTM Model</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#2656a3;\">üëÜ Feature Selection</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features and target\n",
    "features = df.drop(columns=['dk1_spotpricedkk_kwh','datetime','date','timestamp'])\n",
    "target = df['dk1_spotpricedkk_kwh']\n",
    "\n",
    "# Normalize the features\n",
    "scaler = MinMaxScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Normalize the target\n",
    "target = target.values.reshape(-1, 1)\n",
    "target_scaler = MinMaxScaler()\n",
    "target_scaled = target_scaler.fit_transform(target)\n",
    "\n",
    "# Convert back to DataFrame for easier handling\n",
    "features_scaled = pd.DataFrame(features_scaled, index=features.index, columns=features.columns)\n",
    "target_scaled = pd.DataFrame(target_scaled, index=features.index, columns=['dk1_spotpricedkk_kwh'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#2656a3;\"> ‚õ≥Ô∏è Dataset with train and test splits</span>\n",
    "\n",
    "Here we define our train and test splits for traning the model.\n",
    "\n",
    "Create Sequences for X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def create_sequences(features, target, time_steps=24):\n",
    "    X, y = [], []\n",
    "    for i in range(len(features) - time_steps):\n",
    "        X.append(features.iloc[i:i+time_steps].values)\n",
    "        y.append(target.iloc[i+time_steps].values)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "time_steps = 24  # Use the past 24 hours to predict the next hour\n",
    "X, y = create_sequences(features_scaled, target_scaled, time_steps)\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#2656a3;\">üè† Model Building</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(50, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])), # Set to True for multiple LSTM layers\n",
    "    LSTM(50, return_sequences=False), # Set to False for the last LSTM layer\n",
    "    Dense(1) # Output layer\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#2656a3;\">üí™ Model Training</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=12, batch_size=32, validation_data=(X_test, y_test)) # Increase epochs for better performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style='color:#2656a3'> ‚öñÔ∏è Model Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Inverse transform the predictions and the true values to their original scale\n",
    "y_pred_inverse = target_scaler.inverse_transform(y_pred)\n",
    "y_test_inverse = target_scaler.inverse_transform(y_test)\n",
    "\n",
    "# Calculate performance metrics (e.g., RMSE)\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "# Calculate performance metrics\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test_inverse, y_pred_inverse))\n",
    "mae = mean_absolute_error(y_test_inverse, y_pred_inverse)\n",
    "mse = mean_squared_error(y_test_inverse, y_pred_inverse)\n",
    "r2 = r2_score(y_test_inverse, y_pred_inverse)\n",
    "\n",
    "print(f'RMSE: {np.sqrt(rmse)}')\n",
    "print(f'MAE: {mae}')\n",
    "print(f'MSE: {mse}')\n",
    "print(f'R¬≤: {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting the true values and the predicted values\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(y_test_inverse, label='Actual dk1_spotpricedkk_kwh')\n",
    "plt.plot(y_pred_inverse, label='Predicted dk1_spotpricedkk_kwh')\n",
    "plt.title('Actual vs Predicted dk1_spotpricedkk_kwh')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('dk1_spotpricedkk_kwh')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to calculate permutation feature importance\n",
    "def permutation_feature_importance(model, X_val, y_val, feature_names):\n",
    "    baseline_mse = mean_squared_error(y_val, model.predict(X_val))\n",
    "    importances = []\n",
    "\n",
    "    for col in range(X_val.shape[2]):\n",
    "        X_val_permuted = np.copy(X_val)\n",
    "        np.random.shuffle(X_val_permuted[:, :, col])\n",
    "        permuted_mse = mean_squared_error(y_val, model.predict(X_val_permuted))\n",
    "        importances.append(permuted_mse - baseline_mse)\n",
    "\n",
    "    return np.array(importances), feature_names\n",
    "\n",
    "# Calculate feature importance\n",
    "importances, feature_names = permutation_feature_importance(model, X_test, y_test, features.columns)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(len(importances)), importances, align='center')\n",
    "plt.yticks(range(len(importances)), feature_names)\n",
    "plt.xlabel('Increase in MSE after Permutation')\n",
    "plt.title('Permutation Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#2656a3;\">ü§ñ Making the predictions</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the last 5 predictions and their corresponding actual values\n",
    "last_5_predictions = y_pred_inverse[-5:]\n",
    "last_5_actuals = y_test_inverse[-5:]\n",
    "\n",
    "# Print the last 5 predictions and their actual values\n",
    "print(\"Last 5 Predictions vs Actual Values:\")\n",
    "for i in range(5):\n",
    "    print(f\"Prediction: {last_5_predictions[i][0]:.4f}, Actual: {last_5_actuals[i][0]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#2656a3;\">üß¨ Model 2: Temporal LSTM model</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Assuming your DataFrame is named df\n",
    "\n",
    "\n",
    "# Selecting the relevant features and target\n",
    "features = df.drop(columns=['dk1_spotpricedkk_kwh','datetime','date','timestamp'])\n",
    "\n",
    "target = df['dk1_spotpricedkk_kwh'].values.reshape(-1, 1)\n",
    "\n",
    "# Scaling the features and target\n",
    "scaler_features = MinMaxScaler()\n",
    "scaler_target = MinMaxScaler()\n",
    "\n",
    "features_scaled = scaler_features.fit_transform(features)\n",
    "target_scaled = scaler_target.fit_transform(target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(features, target, time_steps=24):\n",
    "    X, y = [], []\n",
    "    for i in range(len(features) - time_steps):\n",
    "        X.append(features[i:(i + time_steps)])\n",
    "        y.append(target[i + time_steps])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "time_steps = 24  # For hourly data, 24 time steps correspond to one day\n",
    "X, y = create_sequences(features_scaled, target_scaled, time_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#2656a3;\"> ‚õ≥Ô∏è Dataset with train and test splits</span>\n",
    "\n",
    "Here we define our train and test splits for traning the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "split_ratio = 0.8\n",
    "split_index = int(len(X) * split_ratio)\n",
    "\n",
    "X_train, X_test = X[:split_index], X[split_index:]\n",
    "y_train, y_test = y[:split_index], y[split_index:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#2656a3;\">üè† Model Building</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, return_sequences=True, input_shape=(time_steps, X_train.shape[2])))\n",
    "model.add(LSTM(50, return_sequences=False))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#2656a3;\">üí™ Model Training</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style='color:#2656a3'> ‚öñÔ∏è Model Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model.evaluate(X_test, y_test)\n",
    "print(f'Val Loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Inverse transform the predictions and the true values to their original scale\n",
    "y_pred_inverse = scaler_target.inverse_transform(y_pred)\n",
    "y_test_inverse = scaler_target.inverse_transform(y_test)\n",
    "\n",
    "# Calculate performance metrics (e.g., RMSE)\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "# Calculate performance metrics\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test_inverse, y_pred_inverse))\n",
    "mae = mean_absolute_error(y_test_inverse, y_pred_inverse)\n",
    "mse = mean_squared_error(y_test_inverse, y_pred_inverse)\n",
    "r2 = r2_score(y_test_inverse, y_pred_inverse)\n",
    "\n",
    "print(f'RMSE: {np.sqrt(rmse)}')\n",
    "print(f'MAE: {mae}')\n",
    "print(f'MSE: {mse}')\n",
    "print(f'R2: {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting the true values and the predicted values\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(y_test_inverse, label='Actual dk1_spotpricedkk_kwh')\n",
    "plt.plot(y_pred_inverse, label='Predicted dk1_spotpricedkk_kwh')\n",
    "plt.title('Actual vs Predicted dk1_spotpricedkk_kwh')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('dk1_spotpricedkk_kwh')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to calculate permutation feature importance\n",
    "def permutation_feature_importance(model, X_val, y_val, feature_names):\n",
    "    baseline_mse = mean_squared_error(y_val, model.predict(X_val))\n",
    "    importances = []\n",
    "\n",
    "    for col in range(X_val.shape[2]):\n",
    "        X_val_permuted = np.copy(X_val)\n",
    "        np.random.shuffle(X_val_permuted[:, :, col])\n",
    "        permuted_mse = mean_squared_error(y_val, model.predict(X_val_permuted))\n",
    "        importances.append(permuted_mse - baseline_mse)\n",
    "\n",
    "    return np.array(importances), feature_names\n",
    "\n",
    "# Calculate feature importance\n",
    "importances, feature_names = permutation_feature_importance(model, X_test, y_test, features.columns)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(len(importances)), importances, align='center')\n",
    "plt.yticks(range(len(importances)), feature_names)\n",
    "plt.xlabel('Increase in MSE after Permutation')\n",
    "plt.title('Permutation Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#2656a3;\">ü§ñ Making the predictions</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the last 5 predictions and their corresponding actual values\n",
    "last_5_predictions = y_pred_inverse[-5:]\n",
    "last_5_actuals = y_test_inverse[-5:]\n",
    "\n",
    "# Print the last 5 predictions and their actual values\n",
    "print(\"Last 5 Predictions vs Actual Values:\")\n",
    "for i in range(5):\n",
    "    print(f\"Prediction: {last_5_predictions[i][0]:.4f}, Actual: {last_5_actuals[i][0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the last 5 predictions and their corresponding actual values\n",
    "last_5_predictions = y_pred_inverse[-5:]\n",
    "last_5_actuals = y_test_inverse[-5:]\n",
    "\n",
    "# Print the last 5 predictions and their actual values\n",
    "print(\"Last 5 Predictions vs Actual Values:\")\n",
    "for i in range(5):\n",
    "    print(f\"Prediction: {last_5_predictions[i][0]:.4f}, Actual: {last_5_actuals[i][0]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#2656a3;\">üß¨ Model 3: Hybrid Conv1D-Bidirectional LSTM Time Series Model</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Conv1D, BatchNormalization, LeakyReLU, MaxPooling1D, Bidirectional, Dropout\n",
    "\n",
    "def build_model(input_dim):\n",
    "    # Creating a Sequential model\n",
    "    model = Sequential()\n",
    "\n",
    "    # Adding a 1D convolutional layer\n",
    "    model.add(Conv1D(filters=64, # Number of filters for the convolutional layer\n",
    "                     kernel_size=1, # kernel size to detect patterns over short periods.\n",
    "                     padding='same', # padding to ensure the output has the same length as the input\n",
    "                     kernel_initializer=\"uniform\", # kernel initializer\n",
    "                     input_shape=(input_dim[0], input_dim[1]))) # input shape\n",
    "    model.add(BatchNormalization()) # normalize the activations of the previous layer at each batch\n",
    "    model.add(LeakyReLU(alpha=0.2)) #  type of activation function based on a ReLU, but it has a small slope for negative values instead of a flat slope.\n",
    "\n",
    "    # Adding 1D convolutional layer\n",
    "    model.add(Conv1D(filters=32, # smaller number of filters for\n",
    "                     kernel_size=1, \n",
    "                     padding='same', \n",
    "                     kernel_initializer=\"uniform\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2)) # alpha 0.2 that is used to control the amount of leakage\n",
    "\n",
    "    # Adding 1D convolutional layer\n",
    "    model.add(Conv1D(filters=16, \n",
    "                     kernel_size=1, \n",
    "                     padding='same', \n",
    "                     kernel_initializer=\"uniform\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    # Adding a 1D max pooling layer\n",
    "    model.add(MaxPooling1D(pool_size=1, padding='same')) # max pooling layer to downsample the input representation\n",
    "\n",
    "    # Adding a Bidirectional LSTM layer\n",
    "    model.add(Bidirectional(LSTM(units=50, return_sequences=True))) # Bidirectional layer to learn from the input sequence in both forward and backward directions\n",
    "    model.add(Dropout(rate=0.1))\n",
    "\n",
    "    # Adding a second LSTM layer\n",
    "    model.add(LSTM(units=50, return_sequences=False))\n",
    "    \n",
    "    # Adding a Dense layer with 1 unit for the output\n",
    "    model.add(Dense(units=1))  # Output layer\n",
    "\n",
    "    # Displaying the model summary\n",
    "    model.summary()\n",
    "\n",
    "    # Compiling the model with mean squared error loss and the Adam optimizer\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#2656a3;\">üè† Model Building</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_dim\n",
    "input_dim = (X_train.shape[1], X_train.shape[2]) # sequence_length, num_features\n",
    "model = build_model(input_dim) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#2656a3;\">üí™ Model Training</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "# history for the loss function and the validation loss function\n",
    "history = model.fit(X_train, y_train, epochs=12, batch_size=32, validation_data=(X_test, y_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the training history dictionary from the model training\n",
    "history_dict = history.history\n",
    "\n",
    "# Displaying the keys in the history dictionary\n",
    "print(history_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting training and validation loss values from the history dictionary\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "\n",
    "# Creating separate variables for loss values (50 epochs)\n",
    "loss_values50 = loss_values\n",
    "val_loss_values50 = val_loss_values\n",
    "\n",
    "# Generating a plot for training and validation loss over epochs\n",
    "epochs = range(1, len(loss_values50) + 1)\n",
    "plt.plot(epochs, loss_values50, 'b', color='blue', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values50, 'b', color='red', label='Validation loss')\n",
    "\n",
    "# Setting plot details and labels\n",
    "plt.rc('font', size=18)\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.xticks(epochs)\n",
    "\n",
    "# Adjusting the size of the plot\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 7)\n",
    "\n",
    "# Displaying the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Inverse transform the predictions and the true values to their original scale\n",
    "y_pred_inverse = scaler_target.inverse_transform(y_pred)\n",
    "y_test_inverse = scaler_target.inverse_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting the true values and the predicted values\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(y_test_inverse, label='Actual dk1_spotpricedkk_kwh')\n",
    "plt.plot(y_pred_inverse, label='Predicted dk1_spotpricedkk_kwh')\n",
    "plt.title('Actual vs Predicted dk1_spotpricedkk_kwh')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('dk1_spotpricedkk_kwh')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style='color:#2656a3'> ‚öñÔ∏è Model Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate performance metrics\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test_inverse, y_pred_inverse))\n",
    "mae = mean_absolute_error(y_test_inverse, y_pred_inverse)\n",
    "mse = mean_squared_error(y_test_inverse, y_pred_inverse)\n",
    "r2 = r2_score(y_test_inverse, y_pred_inverse)\n",
    "\n",
    "print(f'RMSE: {np.sqrt(rmse)}')\n",
    "print(f'MAE: {mae}')\n",
    "print(f'MSE: {mse}')\n",
    "print(f'R2: {r2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#2656a3;\">ü§ñ Making the predictions</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the last 5 predictions and their corresponding actual values\n",
    "last_5_predictions = y_pred_inverse[-5:]\n",
    "last_5_actuals = y_test_inverse[-5:]\n",
    "\n",
    "# Print the last 5 predictions and their actual values\n",
    "print(\"Last 5 Predictions vs Actual Values:\")\n",
    "for i in range(5):\n",
    "    print(f\"Prediction: {last_5_predictions[i][0]:.4f}, Actual: {last_5_actuals[i][0]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#2656a3;\"> Hypertuning the last model</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the Tuner and Set up the RandomSearch tuner with the hypermodel:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#2656a3;\"> ‚õ≥Ô∏è Dataset with train and test splits</span>\n",
    "\n",
    "Here we define our train and test splits for traning the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#2656a3;\">üóÉ Window timeseries</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this case, let‚Äôs assume that given the past 10 days observation, we need to forecast the next 5 days observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color:#2656a3'>üóÑ Model Registry</span>\n",
    "\n",
    "The Model Registry in Hopsworks enable us to store the trained model. The model registry centralizes model management, enabling models to be securely accessed and governed. We can also save model metrics with the model, enabling the user to understand performance of the model on test (or unseen) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting the trained model to a directory\n",
    "model_dir = \"lstm_electricity_price_model\"\n",
    "print('Exporting trained model to: {}'.format(model_dir))\n",
    "\n",
    "# Saving the model using TensorFlow's saved_model.save function\n",
    "tf.saved_model.save(model, model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#2656a3;\">‚öôÔ∏è Model Schema</span>\n",
    "A model schema defines the structure and format of the input and output data that a machine learning model expects and produces, respectively. It serves as a **blueprint** for understanding how to interact with the model in terms of input features and output predictions. In the context of the Hopsworks platform, a model schema is typically defined using the Schema class, which specifies the features expected in the input data and the target variable in the output data. This schema helps ensure consistency and compatibility between the model and the data it operates on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries for saving the model\n",
    "from hsml.schema import Schema\n",
    "from hsml.model_schema import ModelSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Specify the schema of the model's input and output using the features (X_train) and dependent variable (y_train)\n",
    "# input_schema = Schema(X_train)\n",
    "# output_schema = Schema(y_train)\n",
    "\n",
    "# # Create a model schema using the input and output schemas\n",
    "# model_schema = ModelSchema(input_schema, output_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving the Model Registry\n",
    "mr = project.get_model_registry()\n",
    "\n",
    "# Extracting loss value from the training history\n",
    "metrics = {'loss': history_dict['val_loss'][0]} \n",
    "\n",
    "# Creating a TensorFlow model in the Model Registry\n",
    "tf_model = mr.tensorflow.create_model(\n",
    "    name=\"lstm_electricity_price_model\",\n",
    "    metrics=metrics,\n",
    "    #model_schema=model_schema,\n",
    "    description=\"LSTM Daily electricity price prediction model.\",\n",
    "    #input_example=X_train[:1]\n",
    ")\n",
    "\n",
    "# Saving the model to the specified directory\n",
    "tf_model.save(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#2656a3;\">‚è≠Ô∏è **Next:** Part 04: Batch Inference </span>\n",
    "\n",
    "Next notebook we will use the registered model to make predictions based on the batch data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bds-mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
