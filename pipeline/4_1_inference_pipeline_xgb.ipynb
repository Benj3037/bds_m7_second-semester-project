{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"font-width:bold; font-size: 3rem; color:#2656a3;\">**Msc. BDS - M7 Second Semester Project** </span> <span style=\"font-width:bold; font-size: 3rem; color:#333;\">- Part 04: Inference Pipeline</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color:#2656a3'> üóíÔ∏è This notebook is divided into the following sections:\n",
    "\n",
    "1. Load new data for final predictions.\n",
    "2. Predict using model from Model Registry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color:#2656a3'> ‚öôÔ∏è Import of libraries and packages\n",
    "\n",
    "We start by accessing the folder we have created that holds the functions (incl. live API calls and data preprocessing) we need for the weather measures and calendar. Then, we proceed to import some of the necessary libraries needed for this notebook and warnings to avoid unnecessary distractions and keep output clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/tobiasmjensen/Documents/aau_bds/m7_second-semester-project/bds_m7_second-semester-project\n",
      "/Users/tobiasmjensen/Documents/aau_bds/m7_second-semester-project/bds_m7_second-semester-project/pipeline\n"
     ]
    }
   ],
   "source": [
    "# First we go one back in our directory to access the folder with our functions\n",
    "%cd ..\n",
    "\n",
    "# Now we import the functions from the features folder\n",
    "# This is the functions we have created to generate features for weather measures and calandar\n",
    "from features import weather_measures, calendar \n",
    "\n",
    "# We go back into the notebooks folder\n",
    "%cd pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing pandas for data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#2656a3;\"> üì° Connecting to Hopsworks Feature Store\n",
    "We connect to the Hopsworks Feature Store so we can retrieve the Feature View, access the Model Registry, and retrieve the saved model from training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/554133\n",
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    }
   ],
   "source": [
    "# Importing the hopsworks module\n",
    "import hopsworks\n",
    "\n",
    "# Logging into the Hopsworks project\n",
    "project = hopsworks.login()\n",
    "\n",
    "# Getting the feature store from the project\n",
    "fs = project.get_feature_store() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color:#2656a3'> ‚öôÔ∏è Feature View Retrieval\n",
    "We retrieve the Feature View and the training data from the Feature View `electricity_price_fw`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the 'electricity_price_feature_view' feature view\n",
    "electricity_price_fw = fs.get_feature_view(\n",
    "    name='electricity_price_feature_view',\n",
    "    version=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Reading data from Hopsworks, using ArrowFlight           \n"
     ]
    },
    {
     "ename": "FeatureStoreException",
     "evalue": "Could not read data using ArrowFlight. If the issue persists, use read_options={\"use_hive\": True} instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/bds-mlops/lib/python3.11/site-packages/hsfs/core/arrow_flight_client.py:191\u001b[0m, in \u001b[0;36mArrowFlightClient._handle_afs_exception.<locals>.decorator.<locals>.afs_error_handler_wrapper\u001b[0;34m(instance, *args, **kw)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/bds-mlops/lib/python3.11/site-packages/hsfs/core/arrow_flight_client.py:228\u001b[0m, in \u001b[0;36mArrowFlightClient.read_query\u001b[0;34m(self, query_object, arrow_flight_config)\u001b[0m\n\u001b[1;32m    227\u001b[0m descriptor \u001b[38;5;241m=\u001b[39m pyarrow\u001b[38;5;241m.\u001b[39mflight\u001b[38;5;241m.\u001b[39mFlightDescriptor\u001b[38;5;241m.\u001b[39mfor_command(query_encoded)\n\u001b[0;32m--> 228\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescriptor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m    \u001b[49m\u001b[43marrow_flight_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtimeout\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43marrow_flight_config\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDEFAULT_TIMEOUT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/bds-mlops/lib/python3.11/site-packages/hsfs/core/arrow_flight_client.py:222\u001b[0m, in \u001b[0;36mArrowFlightClient._get_dataset\u001b[0;34m(self, descriptor, timeout)\u001b[0m\n\u001b[1;32m    221\u001b[0m reader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mdo_get(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_to_ticket(info), options)\n\u001b[0;32m--> 222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/bds-mlops/lib/python3.11/site-packages/pyarrow/ipc.pxi:618\u001b[0m, in \u001b[0;36mpyarrow.lib._ReadPandasMixin.read_pandas\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/bds-mlops/lib/python3.11/site-packages/pyarrow/array.pxi:883\u001b[0m, in \u001b[0;36mpyarrow.lib._PandasConvertible.to_pandas\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/bds-mlops/lib/python3.11/site-packages/pyarrow/table.pxi:4251\u001b[0m, in \u001b[0;36mpyarrow.lib.Table._to_pandas\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/bds-mlops/lib/python3.11/site-packages/pyarrow/pandas_compat.py:776\u001b[0m, in \u001b[0;36mtable_to_dataframe\u001b[0;34m(options, table, categories, ignore_metadata, types_mapper)\u001b[0m\n\u001b[1;32m    775\u001b[0m _check_data_column_metadata_consistency(all_columns)\n\u001b[0;32m--> 776\u001b[0m columns \u001b[38;5;241m=\u001b[39m \u001b[43m_deserialize_column_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_indexes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    777\u001b[0m blocks \u001b[38;5;241m=\u001b[39m _table_to_blocks(options, table, categories, ext_columns_dtypes)\n",
      "File \u001b[0;32m~/anaconda3/envs/bds-mlops/lib/python3.11/site-packages/pyarrow/pandas_compat.py:901\u001b[0m, in \u001b[0;36m_deserialize_column_index\u001b[0;34m(block_table, all_columns, column_indexes)\u001b[0m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;66;03m# ARROW-1751: flatten a single level column MultiIndex for pandas 0.21.0\u001b[39;00m\n\u001b[0;32m--> 901\u001b[0m columns \u001b[38;5;241m=\u001b[39m \u001b[43m_flatten_single_level_multiindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m columns\n",
      "File \u001b[0;32m~/anaconda3/envs/bds-mlops/lib/python3.11/site-packages/pyarrow/pandas_compat.py:1147\u001b[0m, in \u001b[0;36m_flatten_single_level_multiindex\u001b[0;34m(index)\u001b[0m\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m index\u001b[38;5;241m.\u001b[39mis_unique:\n\u001b[0;32m-> 1147\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFound non-unique column index\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mIndex(\n\u001b[1;32m   1150\u001b[0m     [levels[_label] \u001b[38;5;28;01mif\u001b[39;00m _label \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _label \u001b[38;5;129;01min\u001b[39;00m labels],\n\u001b[1;32m   1151\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   1152\u001b[0m     name\u001b[38;5;241m=\u001b[39mindex\u001b[38;5;241m.\u001b[39mnames[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1153\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Found non-unique column index",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mFeatureStoreException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Retrieve training data from the feature view 'electricity_price_fw', using the training_data() function from the feature_view_training module\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df, _ \u001b[38;5;241m=\u001b[39m \u001b[43melectricity_price_fw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mElectricity Prices Training Dataset\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/bds-mlops/lib/python3.11/site-packages/hsfs/usage.py:208\u001b[0m, in \u001b[0;36mmethod_logger.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    207\u001b[0m     exception \u001b[38;5;241m=\u001b[39m e\n\u001b[0;32m--> 208\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/bds-mlops/lib/python3.11/site-packages/hsfs/usage.py:204\u001b[0m, in \u001b[0;36mmethod_logger.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    201\u001b[0m exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;66;03m# Call the original method\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/bds-mlops/lib/python3.11/site-packages/hsfs/feature_view.py:2219\u001b[0m, in \u001b[0;36mFeatureView.training_data\u001b[0;34m(self, start_time, end_time, description, extra_filter, statistics_config, read_options, spine, primary_keys, event_time, training_helper_columns)\u001b[0m\n\u001b[1;32m   2106\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2107\u001b[0m \u001b[38;5;124;03mCreate the metadata for a training dataset and get the corresponding training data from the offline feature store.\u001b[39;00m\n\u001b[1;32m   2108\u001b[0m \u001b[38;5;124;03mThis returns the training data in memory and does not materialise data in storage.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2202\u001b[0m \u001b[38;5;124;03m    (X, y): Tuple of dataframe of features and labels. If there are no labels, y returns `None`.\u001b[39;00m\n\u001b[1;32m   2203\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2204\u001b[0m td \u001b[38;5;241m=\u001b[39m training_dataset\u001b[38;5;241m.\u001b[39mTrainingDataset(\n\u001b[1;32m   2205\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m   2206\u001b[0m     version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2217\u001b[0m     extra_filter\u001b[38;5;241m=\u001b[39mextra_filter,\n\u001b[1;32m   2218\u001b[0m )\n\u001b[0;32m-> 2219\u001b[0m td, df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_feature_view_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_training_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2220\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mread_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_dataset_obj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprimary_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprimary_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevent_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevent_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_helper_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_helper_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2227\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2228\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   2229\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented version to `\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(td\u001b[38;5;241m.\u001b[39mversion),\n\u001b[1;32m   2230\u001b[0m     util\u001b[38;5;241m.\u001b[39mVersionWarning,\n\u001b[1;32m   2231\u001b[0m )\n\u001b[1;32m   2232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m~/anaconda3/envs/bds-mlops/lib/python3.11/site-packages/hsfs/core/feature_view_engine.py:346\u001b[0m, in \u001b[0;36mFeatureViewEngine.get_training_data\u001b[0;34m(self, feature_view_obj, read_options, splits, training_dataset_obj, training_dataset_version, spine, primary_keys, event_time, training_helper_columns)\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_feature_group_accessibility(feature_view_obj)\n\u001b[1;32m    334\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_batch_query(\n\u001b[1;32m    335\u001b[0m         feature_view_obj,\n\u001b[1;32m    336\u001b[0m         training_dataset_version\u001b[38;5;241m=\u001b[39mtd_updated\u001b[38;5;241m.\u001b[39mversion,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    344\u001b[0m         spine\u001b[38;5;241m=\u001b[39mspine,\n\u001b[1;32m    345\u001b[0m     )\n\u001b[0;32m--> 346\u001b[0m     split_df \u001b[38;5;241m=\u001b[39m \u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_training_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtd_updated\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_view_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread_options\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_training_dataset_statistics(\n\u001b[1;32m    350\u001b[0m         feature_view_obj, td_updated, split_df\n\u001b[1;32m    351\u001b[0m     )\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# split df into features and labels df\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/bds-mlops/lib/python3.11/site-packages/hsfs/engine/python.py:634\u001b[0m, in \u001b[0;36mEngine.get_training_data\u001b[0;34m(self, training_dataset_obj, feature_view_obj, query_obj, read_options)\u001b[0m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_transform_split_df(\n\u001b[1;32m    631\u001b[0m         query_obj, training_dataset_obj, feature_view_obj, read_options\n\u001b[1;32m    632\u001b[0m     )\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 634\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mquery_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mread_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m     transformation_function_engine\u001b[38;5;241m.\u001b[39mTransformationFunctionEngine\u001b[38;5;241m.\u001b[39mpopulate_builtin_transformation_functions(\n\u001b[1;32m    636\u001b[0m         training_dataset_obj, feature_view_obj, df\n\u001b[1;32m    637\u001b[0m     )\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_transformation_function(\n\u001b[1;32m    639\u001b[0m         training_dataset_obj\u001b[38;5;241m.\u001b[39mtransformation_functions, df\n\u001b[1;32m    640\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/bds-mlops/lib/python3.11/site-packages/hsfs/constructor/query.py:173\u001b[0m, in \u001b[0;36mQuery.read\u001b[0;34m(self, online, dataframe_type, read_options)\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjoins) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [f\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m schema]:\n\u001b[1;32m    169\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    170\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPandas types casting only supported for feature_group.read()/query.select_all()\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    171\u001b[0m         )\n\u001b[0;32m--> 173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43msql_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_feature_store_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43monline_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataframe_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mread_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/bds-mlops/lib/python3.11/site-packages/hsfs/engine/python.py:138\u001b[0m, in \u001b[0;36mEngine.sql\u001b[0;34m(self, sql_query, feature_store, online_conn, dataframe_type, read_options, schema)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msql\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    130\u001b[0m     sql_query,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m     schema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    136\u001b[0m ):\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m online_conn:\n\u001b[0;32m--> 138\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sql_offline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m            \u001b[49m\u001b[43msql_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfeature_store\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdataframe_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m            \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhive_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread_options\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhive_config\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mread_options\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m            \u001b[49m\u001b[43marrow_flight_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread_options\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43marrow_flight_config\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mread_options\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdbc(\n\u001b[1;32m    150\u001b[0m             sql_query, online_conn, dataframe_type, read_options, schema\n\u001b[1;32m    151\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/envs/bds-mlops/lib/python3.11/site-packages/hsfs/engine/python.py:168\u001b[0m, in \u001b[0;36mEngine._sql_offline\u001b[0;34m(self, sql_query, feature_store, dataframe_type, schema, hive_config, arrow_flight_config)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sql_offline\u001b[39m(\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    160\u001b[0m     sql_query,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    165\u001b[0m     arrow_flight_config\u001b[38;5;241m=\u001b[39m{},\n\u001b[1;32m    166\u001b[0m ):\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m arrow_flight_client\u001b[38;5;241m.\u001b[39mget_instance()\u001b[38;5;241m.\u001b[39mis_flyingduck_query_object(sql_query):\n\u001b[0;32m--> 168\u001b[0m         result_df \u001b[38;5;241m=\u001b[39m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_with_loading_animation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mReading data from Hopsworks, using ArrowFlight\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m            \u001b[49m\u001b[43marrow_flight_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m            \u001b[49m\u001b[43msql_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m            \u001b[49m\u001b[43marrow_flight_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    175\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_hive_connection(\n\u001b[1;32m    176\u001b[0m             feature_store, hive_config\u001b[38;5;241m=\u001b[39mhive_config\n\u001b[1;32m    177\u001b[0m         ) \u001b[38;5;28;01mas\u001b[39;00m hive_conn:\n\u001b[1;32m    178\u001b[0m             \u001b[38;5;66;03m# Suppress SQLAlchemy pandas warning\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/bds-mlops/lib/python3.11/site-packages/hsfs/util.py:413\u001b[0m, in \u001b[0;36mrun_with_loading_animation\u001b[0;34m(message, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    410\u001b[0m end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 413\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    414\u001b[0m     end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    415\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/bds-mlops/lib/python3.11/site-packages/hsfs/core/arrow_flight_client.py:201\u001b[0m, in \u001b[0;36mArrowFlightClient._handle_afs_exception.<locals>.decorator.<locals>.afs_error_handler_wrapper\u001b[0;34m(instance, *args, **kw)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(instance, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m FeatureStoreException(user_message) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mFeatureStoreException\u001b[0m: Could not read data using ArrowFlight. If the issue persists, use read_options={\"use_hive\": True} instead."
     ]
    }
   ],
   "source": [
    "# Retrieve training data from the feature view 'electricity_price_fw', using the training_data() function from the feature_view_training module\n",
    "df, _ = electricity_price_fw.training_data(\n",
    "    description = 'Electricity Prices Training Dataset',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color:#2656a3'> üìÆ Retrieving Model from Model Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing joblib to load the model\n",
    "import joblib\n",
    "\n",
    "# Retrieve the model registry\n",
    "mr = project.get_model_registry()\n",
    "\n",
    "# Retrieving the model from the Model Registry\n",
    "retrieved_model = mr.get_model(\n",
    "    name=\"xgb_electricity_price_model\", \n",
    "    version=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading the saved models to local directories\n",
    "saved_model_dir = retrieved_model.download()\n",
    "\n",
    "# Loading the saved XGBoost Regressor models\n",
    "retrieved_xgboost_model = joblib.load(saved_model_dir + \"/xgb_electricity_price_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the retrieved XGBoost Regressor model\n",
    "retrieved_xgboost_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color:#2656a3'> ‚ú® Load New Data\n",
    "Our objective is to predict the electricity prices for the upcoming days, therefore we load a weather forecast as new data to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching weather forecast measures for the next 5 days\n",
    "weather_forecast_df = weather_measures.forecast_weather_measures(\n",
    "    forecast_length=5\n",
    ")\n",
    "\n",
    "# Fetching danish calendar\n",
    "calendar_df = calendar.calendar_denmark(\n",
    "    freq='D',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the weather forecast and calendar dataframes\n",
    "new_data = pd.merge(weather_forecast_df, calendar_df, how='inner', left_on='timestamp', right_on='timestamp')\n",
    "\n",
    "# Displaying the new data\n",
    "new_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show information of the new data\n",
    "new_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping and renaming columns\n",
    "new_data.drop(columns=['datetime_y', 'hour_y', 'date_y'], inplace=True)\n",
    "new_data.rename(columns={\n",
    "    'date_x': 'date', \n",
    "    'datetime_x': 'datetime', \n",
    "    'hour_x': 'hour'}, inplace=True)\n",
    "\n",
    "# Displaying the new data\n",
    "new_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show information of the new data after dropping and renaming columns\n",
    "new_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make predictions based on the new date, we must match it to the schema of the training data. We start by sorting the training data retrieved from the feature store based on the timestamp and then use the append method in Pandas to concatenate the two DataFrames. After combining the dataframes we can add columns of moving windows to the combined data to match the schema of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the retrived training data from the feature view based on the values in the 'timestamp' column in ascending order\n",
    "df.sort_values(by='timestamp', ascending=True, inplace=True)\n",
    "\n",
    "# Reset the index of the DataFrame 'df'\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Display the last few rows of the DataFrame 'df' after sorting and resetting the index\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append new data to the historical data stored in the DataFrame 'df'\n",
    "combined_data = df.append(new_data, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the last few rows of the combined data\n",
    "combined_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a copy of the combined data to avoid modifying the original dataframe\n",
    "rolled_data = combined_data.copy()\n",
    "\n",
    "# Adding a column with the mean for the previous 1 week\n",
    "rolled_data['prev_1w_mean'] = rolled_data['dk1_spotpricedkk_kwh'].rolling(window=24*7, min_periods=1).mean()\n",
    "\n",
    "# Adding a column with the mean for the previous 2 weeks\n",
    "rolled_data['prev_2w_mean'] = rolled_data['dk1_spotpricedkk_kwh'].rolling(window=24*14, min_periods=1).mean()\n",
    "\n",
    "# Adding a column with the mean for the previous 4 weeks\n",
    "rolled_data['prev_4w_mean'] = rolled_data['dk1_spotpricedkk_kwh'].rolling(window=24*28, min_periods=1).mean()\n",
    "\n",
    "# Displaying the last 120 rows of the DataFrame\n",
    "rolled_data.tail(120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the last 120 rows of the DataFrame 'rolled_data' and store them in 'forecast_data'\n",
    "forecast_data = rolled_data.tail(120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of the DataFrame 'forecast_data'\n",
    "forecast_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns 'dk1_spotpricedkk_kwh', 'timestamp', 'datetime', and 'date' from the DataFrame 'forecast_data'\n",
    "forecast_data.drop(columns=['dk1_spotpricedkk_kwh','timestamp', 'datetime', 'date'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of the DataFrame 'forecast_data'\n",
    "forecast_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#2656a3;\">ü§ñ Making the predictions</span>\n",
    "\n",
    "We now want to make predictions based on our trained model from Hopsworks and the forecasted weather measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions using the retrieved XGBoost model on the 'forecast_data'\n",
    "# The 'predict' method is called on the 'retrieved_xgboost_model' object, passing 'forecast_data' as input\n",
    "predictions = retrieved_xgboost_model.predict(forecast_data)\n",
    "\n",
    "# Print predictions\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dictionary of the predictions and the corresponding time for the forecast weather measures\n",
    "predictions_data = {\n",
    "    'prediction': predictions,\n",
    "    'time': new_data[\"datetime\"],\n",
    "}\n",
    "\n",
    "# Create a DataFrame from the predictions data\n",
    "predictions_df = pd.DataFrame(predictions_data)\n",
    "\n",
    "# Display the new electricity price predictions\n",
    "predictions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#2656a3;\">üìä Plot the predictions</span>\n",
    "\n",
    "Now we plot the predictions as a linechart for the comming five days.\n",
    "We are plotting the linechart with both matplotlib and Altair for interactive visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing matplotlib for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Group the data by date and calculate the average temperature for each day\n",
    "daily_data = predictions_df.groupby('time')['prediction'].mean()\n",
    " \n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(daily_data.index, daily_data.values)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Spot Price (DKK)')\n",
    "plt.title('Spot Price for DK1 ')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    " \n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Altair for interactive visualization\n",
    "import altair as alt\n",
    "\n",
    "# Create Altair chart with line and dots\n",
    "chart = alt.Chart(predictions_df).mark_line(point=True).encode(\n",
    "    x='time:T', \n",
    "    y='prediction:Q', \n",
    "    # Adding tooltips for interactive information display\n",
    "    tooltip=[alt.Tooltip('time:T', title='Date', format='%Y-%m-%d'), \n",
    "             alt.Tooltip('time:T', title='Time', format='%H:%M'), \n",
    "             alt.Tooltip('prediction:Q', title='Spot Price (DKK)', format='.2f'),\n",
    "            ]\n",
    ")\n",
    " \n",
    "# Display the chart\n",
    "chart.interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <span style=\"color:#2656a3;\">‚è≠Ô∏è **Next:** Creating our Python files for a Streamlit application ([link to Github repository](https://github.com/tobiasmj97/bds_m7_second-semester-project_streamlit)) </span>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bds-mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
